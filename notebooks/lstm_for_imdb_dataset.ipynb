{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4-1 LSTM example for IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ref: https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "\n",
    "Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "\n",
    "## Notes\n",
    "- RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different from what you see with CNNs/MLPs/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix tensorflow GPU allocation\n",
    "\n",
    "#%% GPU memory fix\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "def get_session(gpu_fraction=0.25):    \n",
    "\tgpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction, allow_growth=True)    \n",
    "\treturn tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "keras.backend.set_session(get_session())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "WARNING:tensorflow:From /nfs/home/ymm883/.conda/envs/gpu/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /nfs/home/ymm883/.conda/envs/gpu/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "WARNING:tensorflow:From /nfs/home/ymm883/.conda/envs/gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 133s 5ms/sample - loss: 0.4682 - acc: 0.7789 - val_loss: 0.3941 - val_acc: 0.8260\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 116s 5ms/sample - loss: 0.3031 - acc: 0.8750 - val_loss: 0.3868 - val_acc: 0.8292\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 107s 4ms/sample - loss: 0.2212 - acc: 0.9140 - val_loss: 0.4033 - val_acc: 0.8367\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.1530 - acc: 0.9433 - val_loss: 0.5292 - val_acc: 0.8252\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.1100 - acc: 0.9600 - val_loss: 0.5397 - val_acc: 0.7894\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0835 - acc: 0.9708 - val_loss: 0.6104 - val_acc: 0.8194\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0626 - acc: 0.9795 - val_loss: 0.6982 - val_acc: 0.8221\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0436 - acc: 0.9861 - val_loss: 0.8132 - val_acc: 0.8151\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0323 - acc: 0.9899 - val_loss: 0.8011 - val_acc: 0.8184\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0270 - acc: 0.9914 - val_loss: 0.8839 - val_acc: 0.8204\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0221 - acc: 0.9928 - val_loss: 0.9869 - val_acc: 0.8142\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0187 - acc: 0.9944 - val_loss: 0.9373 - val_acc: 0.8034\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 101s 4ms/sample - loss: 0.0177 - acc: 0.9940 - val_loss: 1.0376 - val_acc: 0.8144\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0153 - acc: 0.9948 - val_loss: 1.0111 - val_acc: 0.8103\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 102s 4ms/sample - loss: 0.0080 - acc: 0.9979 - val_loss: 1.1168 - val_acc: 0.8107\n",
      "25000/25000 [==============================] - 23s 932us/sample - loss: 1.1168 - acc: 0.8107\n",
      "Test score: 1.1167651077328622\n",
      "Test accuracy: 0.81068\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
